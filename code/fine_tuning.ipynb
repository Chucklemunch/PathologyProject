{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f15eb-79d1-4e12-b6f8-473734ca5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from transformers import AutoModel\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW, Adam, SGD\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from PathBinaryClassifier import PathBinaryClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f5e09-9647-4726-9083-122bd9d9605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making datasets\n",
    "def loader(path):\n",
    "    img = Image.open(path)\n",
    "    return img\n",
    "\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(224),\n",
    "        v2.CenterCrop(224),\n",
    "        v2.ToImage(), \n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "    \n",
    "dataset = ImageFolder(\n",
    "    '../images/40X/', \n",
    "    loader=loader,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split data into train/val/test\n",
    "num_imgs = len(dataset.samples)\n",
    "train_size = int(num_imgs * 0.7)\n",
    "val_size = int(num_imgs * 0.15)\n",
    "test_size = num_imgs - train_size - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset=dataset,\n",
    "    lengths=[train_size, val_size, test_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "# Make dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33b778-adc4-4d30-87f8-84a10207016f",
   "metadata": {},
   "source": [
    "# Load Model and Create Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f23236-8d82-42f0-bda2-b2a8803f1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Use GPU if avail\n",
    "# Load model\n",
    "with torch.no_grad():\n",
    "    backbone = AutoModel.from_pretrained('kaiko-ai/midnight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ce220-e645-4536-a16e-529226fabd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify components needed for training\n",
    "\n",
    "# NEED GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Instantiate model\n",
    "model = PathBinaryClassifier(backbone).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b098fb1-9e4f-409b-a512-eb9d2a8e84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except for those in classification head\n",
    "for name, block in model.named_children():\n",
    "    if name != 'classifier':\n",
    "        for param in block.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6373929-e475-49da-8af2-93caf3d0231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify opt and loss function\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad == True, model.parameters()), lr=2e-5)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6384ff-ea15-4f11-8a4a-adf49724f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Wandb config\n",
    "project = 'PathologyFineTuning'\n",
    "config = {\n",
    "    'architecture': 'Midnight-12k',\n",
    "    'dataset': 'BreakHis',\n",
    "    'optimizer': 'AdamW',\n",
    "    'classifier_hidden_dim': 512,\n",
    "    'epochs': 20,\n",
    "}\n",
    "name = 'run-20251120'\n",
    "\n",
    "# Define config for Sweep\n",
    "sweep_config = {\n",
    "    'method' : 'random',\n",
    "    'metric' : {'name': 'val_acc', 'goal': 'maximize'},\n",
    "    'parameters' : {\n",
    "        'classifier_hidden_dim' : {'min': 32, 'max': 2048},\n",
    "        'init_lr' : {'min': 1e-7, 'max': 1e-3},\n",
    "        'dropout' : {'min': 0.1, 'max': 0.5},\n",
    "        'optimizer' : {'values' : ['AdamW', 'Adam', 'SGD']},\n",
    "        'criterion' : {'values' : ['CrossEntropy']}\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4ff9d-7575-44d9-900a-94341df96e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for hyperparameter sweep using Wandb\n",
    "def sweep_train():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        hidden_dim = config.classifier_hidden_dim\n",
    "        opt_choice = config.optimizer\n",
    "        init_lr = config.init_lr\n",
    "        dropout = config.dropout\n",
    "        criterion = config.criterion\n",
    "\n",
    "        # Define model based on HP choice\n",
    "        model = PathBinaryClassifier(\n",
    "            backbone=backbone, \n",
    "            hidden=hidden_dim, \n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        # Freeze model params\n",
    "        for name, block in model.named_children():\n",
    "            if name != 'classifier':\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # Select Loss Function\n",
    "        if criterion == 'CrossEntropy':\n",
    "            loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Setup optimizer based on HP choice\n",
    "        if opt_choice == 'AdamW':\n",
    "            opt = AdamW(filter(lambda p: p.requires_grad == True, model.parameters()), lr=2e-5)\n",
    "        elif opt_choice == 'Adam':\n",
    "            opt = Adam(filter(lambda p: p.requires_grad == True, model.parameters()), lr=2e-5)\n",
    "        elif opt_choice == 'SGD':\n",
    "            opt = SGD(filter(lambda p: p.requires_grad == True, model.parameters()), lr=2e-5)\n",
    "\n",
    "        # Train model with HPs choice by sweep\n",
    "        train(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            val_dataloader,\n",
    "            opt,\n",
    "            loss_fun # defined externally\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c7c889-3b7d-4c9a-8b50-90360fa2cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_with_artifact(model, run):\n",
    "    \"\"\"\n",
    "    Saves model state-dict and logs model to Wandb\n",
    "    \"\"\"\n",
    "\n",
    "    # Save model\n",
    "    path = f'models/PathBinaryClassifier_{run.name}.pt'\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "    # Save artifact to Wandb\n",
    "    artifact = wandb.Artifact('model', type='model')\n",
    "    artifact.add_file(path)\n",
    "    run.log_artifact(artifact)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb434d4-a590-42a2-a980-1648258bb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, opt, l, epochs=20, grad_accum=4):\n",
    "    # Use wandb for tracking\n",
    "    with wandb.init(project=project, config=config, name=name) as run:\n",
    "    # run = wandb.run # if initialized sweep\n",
    "        print('run config: ', run.config)\n",
    "    \n",
    "        epoch_train_loss = []\n",
    "        epoch_train_acc = []\n",
    "        epoch_val_acc = []\n",
    "        best_val_acc = 0\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            running_loss = 0\n",
    "            running_correct_train = 0\n",
    "            running_correct_val = 0\n",
    "            \n",
    "            print(f'---- Epoch {i+1}/{epochs}----')\n",
    "            opt.zero_grad()\n",
    "    \n",
    "            # training\n",
    "            for X, y in tqdm(train_dataloader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "    \n",
    "                # Get predictions\n",
    "                output = model(X)\n",
    "    \n",
    "                # Compute loss and gradients\n",
    "                loss = l(output, y)\n",
    "                running_loss += loss\n",
    "                loss.backward()\n",
    "    \n",
    "                # Count correct predictions\n",
    "                preds = torch.argmax(output, dim=1)\n",
    "                running_correct_train += sum(preds == y).item()\n",
    "                \n",
    "                # Update parameters after {grad_accum} batches\n",
    "                if (i+1) % grad_accum == 0:\n",
    "                    opt.step()\n",
    "                    opt.zero_grad()\n",
    "    \n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "    \n",
    "            # Metrics for training epoch\n",
    "            cur_train_loss = running_loss / len(train_dataloader.dataset)\n",
    "            cur_train_acc = running_correct_train / len(train_dataloader.dataset)\n",
    "            epoch_train_loss.append(cur_train_loss)\n",
    "            epoch_train_acc.append(cur_train_acc)\n",
    "    \n",
    "            print(f'---- Epoch {i+1}/{epochs} Train Loss: {cur_train_loss} --- Train Accuracy: {cur_train_acc} ----')\n",
    "    \n",
    "            # Wandb logging\n",
    "            run.log({'train_loss': cur_train_loss, 'train_acc': cur_train_acc})\n",
    "            \n",
    "            # validation\n",
    "            for X, y in tqdm(val_dataloader):\n",
    "                X, y, = X.to(device), y.to(device)\n",
    "    \n",
    "                # Get predictions\n",
    "                output = model(X)\n",
    "                \n",
    "                # Count correct predictions\n",
    "                preds = torch.argmax(output, dim=1)\n",
    "                \n",
    "                running_correct_val += sum(preds == y).item()\n",
    "    \n",
    "    \n",
    "            # Metrics for validation epoch\n",
    "            cur_val_acc = running_correct_val / len(val_dataloader.dataset)\n",
    "            epoch_val_acc.append(cur_val_acc)\n",
    "            \n",
    "            if cur_val_acc > best_val_acc:\n",
    "                best_val_acc = cur_val_acc            \n",
    "                                                    \n",
    "            print(f'---- Epoch {i+1}/{epochs} Val Accuracy: {cur_val_acc} ----')\n",
    "    \n",
    "            # Wandb logging\n",
    "            run.log({'val_acc': cur_val_acc})\n",
    "        \n",
    "        print('Best val acc: ', best_val_acc)\n",
    "\n",
    "        # Save model and log to Wandb\n",
    "        save_model_with_artifact(model, run)\n",
    "        \n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e41d1-9095-4dc8-aa44-69b9c525f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, best_val_acc = train(model, train_dataloader, val_dataloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf5c55-5143-4bfb-a64f-c9b4cf8317ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77dcaa-fea2-4065-a586-006a883ea63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch sweep\n",
    "wandb.agent(sweep_id, function=sweep_train, count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27b0bf-4724-4d38-8ab5-0a8a1ecfc645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best run from sweep\n",
    "sweep_id = '3bh8rw9f'\n",
    "\n",
    "api = wandb.Api()\n",
    "sweep = api.from_path(f'team-chucklemunch/PathologyFineTuning/sweeps/{sweep_id}')\n",
    "\n",
    "best_config = None\n",
    "best_run = None\n",
    "best_val_acc = 0\n",
    "\n",
    "# Selects best run\n",
    "for run in sweep.runs:\n",
    "    if run.summary['val_acc'] > best_val_acc:\n",
    "        best_run = run\n",
    "        best_val_acc = run.summary['val_acc']\n",
    "\n",
    "# Get config from best run\n",
    "best_config = run.config\n",
    "best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9d9f0e-b7a5-4f62-9f33-20448090f4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
