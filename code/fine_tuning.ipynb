{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f15eb-79d1-4e12-b6f8-473734ca5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from transformers import AutoModel\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW, Adam, SGD\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f5e09-9647-4726-9083-122bd9d9605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making datasets\n",
    "def loader(path):\n",
    "    img = Image.open(path)\n",
    "    return img\n",
    "\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(224),\n",
    "        v2.CenterCrop(224),\n",
    "        v2.ToImage(), \n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ]\n",
    ")\n",
    "    \n",
    "dataset = ImageFolder(\n",
    "    'images/40X/', \n",
    "    loader=loader,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split data into train/val/test\n",
    "num_imgs = len(dataset.samples)\n",
    "train_size = int(num_imgs * 0.7)\n",
    "val_size = int(num_imgs * 0.15)\n",
    "test_size = num_imgs - train_size - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset=dataset,\n",
    "    lengths=[train_size, val_size, test_size],\n",
    "    generator=generator\n",
    ")\n",
    "\n",
    "# Make dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c33b778-adc4-4d30-87f8-84a10207016f",
   "metadata": {},
   "source": [
    "# Load Model and Create Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f23236-8d82-42f0-bda2-b2a8803f1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# Use GPU if avail\n",
    "# Load model\n",
    "with torch.no_grad():\n",
    "    backbone = AutoModel.from_pretrained('kaiko-ai/midnight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ded63-68dc-4e89-98b9-f81c0c449e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathBinaryClassifier(nn.Module):\n",
    "    def __init__(self, backbone, hidden=512, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        self.dim = backbone.config.hidden_size # 1536 for Midnight Model\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Classifier block\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 2), # 1 for BCELoss, 2 for CrossEntropyLoss\n",
    "            # nn.Sigmoid() Needed for BCELoss, but not \n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.backbone(X)['pooler_output']\n",
    "        logit = self.classifier(out)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ce220-e645-4536-a16e-529226fabd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify components needed for training\n",
    "\n",
    "# NEED GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Instantiate model\n",
    "# model = PathBinaryClassifier(backbone).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b098fb1-9e4f-409b-a512-eb9d2a8e84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except for those in classification head\n",
    "# for name, block in model.named_children():\n",
    "#     if name != 'classifier':\n",
    "#         for param in block.parameters():\n",
    "#             param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6373929-e475-49da-8af2-93caf3d0231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify opt and loss function\n",
    "# optimizer = AdamW(filter(lambda p: p.requires_grad == True, model.parameters()), lr=2e-5)\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6384ff-ea15-4f11-8a4a-adf49724f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify Wandb config\n",
    "project = 'PathologyFineTuning'\n",
    "config = {\n",
    "    'architecture': 'Midnight-12k',\n",
    "    'dataset': 'BreakHis',\n",
    "    'optimizer': 'AdamW',\n",
    "    'classifier_hidden_dim': 512,\n",
    "    'epochs': 2,\n",
    "}\n",
    "name = 'trial-run'\n",
    "\n",
    "# Define config for Sweep\n",
    "sweep_config = {\n",
    "    'method' : 'random',\n",
    "    'metric' : {'name': 'val_acc', 'goal': 'maximize'},\n",
    "    'parameters' : {\n",
    "        'classifier_hidden_dim' : {'min': 32, 'max': 2048},\n",
    "        'init_lr' : {'min': 1e-7, 'max': 1e-3},\n",
    "        'dropout' : {'min': 0.1, 'max': 0.5},\n",
    "        'optimizer' : {'values' : ['AdamW', 'Adam', 'SGD']},\n",
    "        'criterion' : {'values' : ['CrossEntropy']}\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4ff9d-7575-44d9-900a-94341df96e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for hyperparameter sweep using Wandb\n",
    "def sweep_train():\n",
    "    with wandb.init() as run:\n",
    "        config = wandb.config\n",
    "        hidden_dim = config.classifier_hidden_dim\n",
    "        opt_choice = config.optimizer\n",
    "        init_lr = config.init_lr\n",
    "        dropout = config.dropout\n",
    "        criterion = config.criterion\n",
    "\n",
    "        # Define model based on HP choice\n",
    "        model = PathBinaryClassifier(\n",
    "            backbone=backbone, \n",
    "            hidden=hidden_dim, \n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        # Freeze model params\n",
    "        for name, block in model.named_children():\n",
    "            if name != 'classifier':\n",
    "                for param in block.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "        # Select Loss Function\n",
    "        if criterion == 'CrossEntropy':\n",
    "            loss_fun = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Setup optimizer based on HP choice\n",
    "        if opt_choice == 'AdamW':\n",
    "            opt = AdamW(filter(lambda p: p.requires_grad == True, model.parameters()), lr=2e-5)\n",
    "        elif opt_choice == 'Adam':\n",
    "            opt = Adam(filter(lambda p: p.requires_grad == True, model.parameters()), lr=2e-5)\n",
    "        elif opt_choice == 'SGD':\n",
    "            opt = SGD(filter(lambda p: p.requires_grad == True, model.parameters()), lr=2e-5)\n",
    "\n",
    "        # Train model with HPs choice by sweep\n",
    "        train(\n",
    "            model,\n",
    "            train_dataloader,\n",
    "            val_dataloader,\n",
    "            opt,\n",
    "            loss_fun # defined externally\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb434d4-a590-42a2-a980-1648258bb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, opt, l, epochs=2, grad_accum=4):\n",
    "    # Use wandb for tracking\n",
    "    # with wandb.init(project=project, config=config, name=name) as run:\n",
    "    run = wandb.run # if initialized sweep\n",
    "    \n",
    "    epoch_train_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_val_acc = []\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        running_loss = 0\n",
    "        running_correct_train = 0\n",
    "        running_correct_val = 0\n",
    "        \n",
    "        print(f'---- Epoch {i+1}/{epochs}----')\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # training\n",
    "        for X, y in tqdm(train_dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            output = model(X)\n",
    "\n",
    "            # Compute loss and gradients\n",
    "            loss = l(output, y)\n",
    "            running_loss += loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Count correct predictions\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            running_correct_train += sum(preds == y).item()\n",
    "            \n",
    "            # Update parameters after {grad_accum} batches\n",
    "            if (i+1) % grad_accum == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        # Metrics for training epoch\n",
    "        cur_train_loss = running_loss / len(train_dataloader.dataset)\n",
    "        cur_train_acc = running_correct_train / len(train_dataloader.dataset)\n",
    "        epoch_train_loss.append(cur_train_loss)\n",
    "        epoch_train_acc.append(cur_train_acc)\n",
    "\n",
    "        print(f'---- Epoch {i+1}/{epochs} Train Loss: {cur_train_loss} --- Train Accuracy: {cur_train_acc} ----')\n",
    "\n",
    "        # Wandb logging\n",
    "        run.log({'train_loss': cur_train_loss, 'train_acc': cur_train_acc})\n",
    "        \n",
    "        # validation\n",
    "        for X, y in tqdm(val_dataloader):\n",
    "            X, y, = X.to(device), y.to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            output = model(X)\n",
    "            \n",
    "            # Count correct predictions\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            \n",
    "            running_correct_val += sum(preds == y).item()\n",
    "\n",
    "\n",
    "        # Metrics for validation epoch\n",
    "        cur_val_acc = running_correct_val / len(val_dataloader.dataset)\n",
    "        epoch_val_acc.append(cur_val_acc)\n",
    "        \n",
    "        if cur_val_acc > best_val_acc:\n",
    "            best_val_acc = cur_val_acc            \n",
    "                                                \n",
    "        print(f'---- Epoch {i+1}/{epochs} Val Accuracy: {cur_val_acc} ----')\n",
    "\n",
    "        # Wandb logging\n",
    "        run.log({'val_acc': cur_val_acc})\n",
    "    \n",
    "    print('Best val acc: ', best_val_acc)\n",
    "    \n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf5c55-5143-4bfb-a64f-c9b4cf8317ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb77dcaa-fea2-4065-a586-006a883ea63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch sweep\n",
    "wandb.agent(sweep_id, function=sweep_train, count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24de726-1211-4424-9fb1-52d87136856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training without doing sweep\n",
    "# model, best_val_acc = train(model, \n",
    "#                             train_dataloader, \n",
    "#                             val_dataloader, \n",
    "#                             optimizer, \n",
    "#                             criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
